version: "3.8"
services:
  zookeeper:
    image: confluentinc/cp-zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
  kafka:
    image: confluentinc/cp-kafka
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
  # The only purpose of this service is to wait for the Kafka broker to start and create the topics
  # https://github.com/confluentinc/examples/blob/5.4.11-post/microservices-orders/docker-compose.yml#L209
  kafka-setup:
    image: confluentinc/cp-kafka
    depends_on:
      - kafka
    entrypoint: ["bash", "-c", "echo Waiting for Kafka to be ready... && \
              cub kafka-ready -b kafka:9092 1 40 && \
              kafka-topics --create --if-not-exists --bootstrap-server kafka:9092 --partitions 1 --replication-factor 1 --topic ${KAFKA_TOPIC}"]
  iot-telemetry-simulator:
    image: mcr.microsoft.com/oss/azure-samples/azureiot-telemetrysimulator
    depends_on:
      - kafka-setup
    environment:
      KafkaConnectionProperties: '{"bootstrap.servers": "kafka:9092"}'
      KafkaTopic: ${KAFKA_TOPIC}
      DeviceCount: 3
      MessageCount: 0 # send unlimited
      Template: '{ "deviceId": "$.DeviceId", "time": "$.Time", "doubleValue": $.DoubleValue }'
      Variables: '[{"name": "DoubleValue", "randomDouble":true, "min":0.22, "max":1.25}]'
  sql-server:
    image: mcr.microsoft.com/mssql/server:2022-latest
    ports:
      - 1433:1433
    environment:
      ACCEPT_EULA: Y
      MSSQL_SA_PASSWORD: ${MSSQL_SA_PASSWORD}
  spark:
    # Until this issue is resolved, we're stuck at Spark 3.1.x
    # https://github.com/microsoft/sql-spark-connector/issues/191
    image: apache/spark-py:v3.1.3
    volumes:
      - ./pyspark_app:/app
      - ./rooms.csv:/metadata/rooms.csv
      # Below 2 directories aren't writable hence these 2 mounts must be created for spark-submit to resolve Maven packages
    depends_on:
      - kafka-setup
      - sql-server
    environment:
      KAFKA_BROKER: kafka:9092
      KAFKA_TOPIC: ${KAFKA_TOPIC}
      MSSQL_HOST: sql-server
      MSSQL_SA_PASSWORD: ${MSSQL_SA_PASSWORD}
    command: ["/opt/spark/bin/spark-submit", 
      "--packages", 
      # There should be no whitespace between the comma separated packages https://stackoverflow.com/questions/33928029/how-to-specify-multiple-dependencies-using-packages-for-spark-submit
      # There is an explicit need to add the mssql-jdbc package https://stackoverflow.com/questions/66903523/apache-spark-connector-for-sql-server-and-azure-sql
      # The default Ivy Cache location is not writable in the container hence have to override it to /tmp https://stackoverflow.com/a/69559038
      "org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.3,com.microsoft.azure:spark-mssql-connector_2.12:1.2.0,com.microsoft.sqlserver:mssql-jdbc:8.4.1.jre8", 
      "--conf", 
      "spark.driver.extraJavaOptions=-Divy.cache.dir=/tmp -Divy.home=/tmp",
      "/app/main.py"]
