# Databricks notebook source
# MAGIC %pip install /dbfs/FileStore/common_lib-0.0.1-py3-none-any.whl

# COMMAND ----------

import dlt
from pyspark.sql import DataFrame
from stream_processor import StreamProcessor

# COMMAND ----------

@dlt.table(
  comment="Mapping data between rooms and devices",
  temporary=True
)
def metadata():
  return (spark.read.csv(spark.conf.get("metadataPath"),
                         header=True,
                         inferSchema=True))

# COMMAND ----------

scope_name = spark.conf.get("secretsScopeName")

eh_name = dbutils.secrets.get(scope_name, "EH_NAME")
eh_namespace = dbutils.secrets.get(scope_name, "EH_NAMESPACE")
eh_connection_string = dbutils.secrets.get(scope_name, "EH_CONNECTION_STRING")

bootstrap_server = f"{eh_namespace}.servicebus.windows.net:9093"
eh_sasl = f'org.apache.kafka.common.security.plain.PlainLoginModule required username="$ConnectionString" password="{eh_connection_string}";'

kafka_options = {
    "kafka.bootstrap.servers": bootstrap_server,
    "subscribe": eh_name,
    "kafka.sasl.mechanism": "PLAIN",
    "kafka.security.protocol": "SASL_SSL",
    "kafka.sasl.jaas.config": eh_sasl
}

# COMMAND ----------

@dlt.table(
  comment="Streaming data generated by the IoT sensors",
  temporary=True
)
def stream_raw():
  return (spark.readStream
          .format("kafka")
          .options(**kafka_options)
          .load())

# COMMAND ----------

processor = StreamProcessor()

@dlt.table(
  comment="Processed stream ready to be served"
)
def stream_processed():
  return processor.process_stream(dlt.read("metadata"),
                                  dlt.read_stream("stream_raw"))
